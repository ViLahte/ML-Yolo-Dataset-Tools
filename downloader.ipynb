{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd00aad04442bd4d4809397607a613ef36be2cf5a5aab58ae393dd51645c2c3ffd0",
   "display_name": "Python 3.8.5  ('venv': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "0aad04442bd4d4809397607a613ef36be2cf5a5aab58ae393dd51645c2c3ffd0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Download images"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "import time\n",
    "import csv\n",
    "\n",
    "from typing import List, Type, Optional, Union, Tuple\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() "
   ]
  },
  {
   "source": [
    "## Configure the downloader"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetName = 'dragons'\n",
    "\n",
    "query = os.getenv('QUERY')\n",
    "base_url = os.getenv('BASE_URL')\n",
    "headers = {\n",
    "    'Accept': 'application/json', \n",
    "    'User-Agent': 'Dragon dataset downloader 0.0.1',\n",
    "}\n",
    "\n",
    "image_download_interval = 1.1\n",
    "json_download_interval = 5\n",
    "pages_start = 1\n",
    "pages_end = 2\n",
    "\n",
    "datasetsRootPath = pathlib.Path('datasets') / datasetName\n",
    "datasetsRootPath.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "loadedImagesListFile = datasetsRootPath / ('loaded_images_' + datasetName + '.csv')\n",
    "\n",
    "queryJsonFile = datasetsRootPath / (query + f'_pages_{pages_start}-{pages_end}' + '.json')\n",
    "\n",
    "downloadFolder = datasetsRootPath / 'data' / 'obj'\n",
    "downloadFolder.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "logging.basicConfig(filename=datasetsRootPath / 'skipped.log', level=logging.INFO)"
   ]
  },
  {
   "source": [
    "## Get definitions by score, for mass scraping, getting by post id range would be better."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "for i in range(pages_start,pages_end):\n",
    "    req = requests.get(base_url+f'/posts?page={i}' + query, headers=headers)\n",
    "    print('Page:', str(i), 'Status code:', str(req.status_code), 'Number of fetched definitions:', str(len(req.json()['posts'])))\n",
    "    lst = lst + req.json()['posts']\n",
    "    time.sleep(json_download_interval)"
   ]
  },
  {
   "source": [
    "## Save definitions into a JSON-file"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(queryJsonFile, \"w\") as outfile: \n",
    "    json.dump(lst, outfile)"
   ]
  },
  {
   "source": [
    "## Load definitions from a JSON-file"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(queryJsonFile) as json_file:\n",
    "    loaded = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxScorePost = max(loaded, key=lambda x:x['score']['total'])\n",
    "minScorePost = min(loaded, key=lambda x:x['score']['total'])\n",
    "maxIDPost = max(loaded, key=lambda x:x['id'])\n",
    "minIDPost = min(loaded, key=lambda x:x['id'])"
   ]
  },
  {
   "source": [
    "## Define the helper methods"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveImageFromPost(post: List[dict], savePath) -> None:\n",
    "    url = post.get('file').get('url')\n",
    "\n",
    "    if url is None:\n",
    "        logging.error(f'Skipping image ID {post[\"id\"]} due to missing image URL')\n",
    "        raise ValueError\n",
    "\n",
    "    filename = url.rsplit('/', 1)[1]\n",
    "    \n",
    "    # Use MD5, ID is also a possibility.\n",
    "    saveImageFromUrl(url,filename, savePath)\n",
    "\n",
    "def saveImageFromUrl(url: str, filename: str, savePath) -> None:\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    if not r.ok:\n",
    "        print(r.status_code)\n",
    "    open(savePath.joinpath(filename), 'wb').write(r.content)\n",
    "\n",
    "def addIdsAndUrlsToCsv(new_ids: List[int], new_urls: List[str], filepath):\n",
    "    with open(filepath, mode='a') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        for new_id, new_url in zip(new_ids,new_urls):\n",
    "            writer.writerow([new_id,new_url])\n",
    "\n",
    "def readAlreadyLoadedSet(filepath) -> set:\n",
    "    '''\n",
    "    Assumes:\n",
    "        - A CSV with two columns interpreted as 'id' and 'url'\n",
    "    '''\n",
    "    already_loaded = set()\n",
    "\n",
    "    with open(filepath, mode='r') as infile:\n",
    "        reader = csv.reader(infile)\n",
    "        data = list(reader)\n",
    "        for row in data:\n",
    "            already_loaded.add(int(row[0]))\n",
    "    return already_loaded\n",
    "\n",
    "def saveDatasetFromListOfPosts(lst, savePath, loadedImagesListFile, already_loaded: Optional[set] = set()):\n",
    "    try:\n",
    "        already_loaded = readAlreadyLoadedSet(loadedImagesListFile)\n",
    "    except:\n",
    "        already_loaded = set()\n",
    "\n",
    "    new_urls = []\n",
    "    new_ids = []\n",
    "\n",
    "    try:\n",
    "        for post in lst:\n",
    "            if post['id'] not in already_loaded:\n",
    "                try:\n",
    "                    saveImageFromPost(post, savePath)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "                new_urls.append(post['file']['url'])\n",
    "                new_ids.append(post['id'])\n",
    "            else:\n",
    "                continue\n",
    "            time.sleep(image_download_interval)\n",
    "    except:\n",
    "        addIdsAndUrlsToCsv(new_ids,new_urls,loadedImagesListFile)\n",
    "        raise\n",
    "    addIdsAndUrlsToCsv(new_ids,new_urls)"
   ]
  },
  {
   "source": [
    "## Save the dataset defined in definitions json, can be interrupted"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveDatasetFromListOfPosts(loaded, downloadFolder, loadedImagesListFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}